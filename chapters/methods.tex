%!TEX root=thesis.tex

\chapter{Methods} % Reinforcement Learning for Hamiltonian Engineering

% TODO make sure it's clear what the goal is: Hamiltonian engineering

% physical constraints
% parameters for sequences

This section describes the simulation methods and RL algorithm used in this work. The implementation of both can be found at \url{https://github.com/wjkaufman/rl_pulse}.

\section{Simulated Spin Systems}

Isolated spin systems composed of a small number of spins can be easily simulated on a classical computer. Because the Hilbert space grows exponentially as the spins increase ($2^N$ basis states for $N$ spins and $2^{2N}$ matrix elements for operators), it quickly becomes impractical to simulate large numbers of spins. To balance the need to capture dipole interactions and to decrease simulation time, $N=3$ spin-1/2 systems were used in the reinforcement learning algorithm, and $N=4$ spin-1/2 systems were used to evaluate pulse sequences. The QuTiP Python package was used for all simulations \cite{Johansson_2013}.

In any real physical system, system-environment interactions are present and the spin system must be treated as open, but for simulation purposes the system is assumed to be isolated.
Consequently, the dynamics are given by unitary time-evolution operators. In principle, though, simulations of open quantum systems could be used with reinforcement learning algorithms, and may in fact lead to better control by accounting for non-unitary system dynamics.

For each simulated $N$-spin system, the chemical shift strengths $\{\delta_j\}_j$ and the dipolar coupling strengths $\{d_{jk}\}_{j,k}$ are drawn from normal distributions
\begin{align*}
    \delta_j &\sim \mathcal{N}(\mu=0, \sigma=1) \\
    d_{jk} &\sim \mathcal{N}(\mu=0, \sigma=D)
\end{align*}
so that the energy scale of the system is defined by the standard deviation of the chemical shifts. The parameter $D$ controls the regime of the spin system, where $D \gg 1$ is a ``strongly coupled'' spin system and $D \ll 1$ is ``weakly coupled.'' We focus on strongly coupled spin systems, and set $D = 100$ unless otherwise specified. With the chemical shift and dipolar coupling strengths, the internal Hamiltonian $H_{\text{int}}$ can be calculated (see equation~\ref{eq:nmr-ham}).

% As an example, in a $3$-spin system, there will be three chemical shift terms, six dipolar coupling terms, and $H_{\text{int}}$ will be an $8
% \times 8$ Hermitian matrix.

Once $H_{\text{int}}$ has been calculated, the free evolution propagator $U(\tau) \defeq U_\tau$ can be calculated via matrix exponentiation (see equation~\ref{eq:propagator-ti})
and
the pulse operators $\{ U_X, U_{\overline{X}}, U_Y, U_{\overline{Y}} \}$ can be calculated using equation~\ref{eq:X-pulse}.

Because the (random) choices of chemical shift and dipolar coupling strengths affect the dynamics of the spin system, an ensemble of spin systems was used both in the reinforcement learning algorithm and in evaluation of pulse sequences.
Each spin system used different random chemical shift and dipolar coupling strengths drawn from the same distributions (i.e. the dipolar strength parameter $D$ was the same for all systems). If experimental imperfections were also included, then each system also used different random errors.
It was found that using ensembles of 50 spin systems reduced the variance in fidelity enough while keeping runtimes relatively short, so
the reinforcement learning algorithm used 50 spin systems and evaluation used 100 spin systems.

To assess the performance of a pulse sequence for a Hamiltonian engineering problem, the pulse sequence propagator is compared to the target propagator given by
\[
U_{\text{target}} = \exp\left\{ -i H_{\text{eff}} T \right\}
\]
Under ideal conditions, the two propagators should be ``close'' in some way. One measure of ``closeness'' is the following fidelity function for two unitary operators
\begin{equation}
    \text{fidelity}(U, U_\text{target}) = \left| \frac{\Tr{
        U^\dagger U_\text{target}
    }}{\Tr{\identity}} \right|
\end{equation}
The fidelity approaches $1$ as the unitaries get ``closer'' and the average Hamiltonian approaches the target Hamiltonian, and approaches $0$ as the unitaries get further apart.

\section{Reinforcement Learning Implementation}\label{sec:methods-RL}

As explained previously,
the reinforcement learning methodology is sufficiently general to be applied to a wide range of problems in a variety of different ways.  The specific application of RL to the Hamiltonian engineering problem is described below.

\subsection{Action Space and Representation}

In an NMR spectrometer, the actions we can perform, or the ``knobs'' we can turn, are the rf field amplitudes in equation~\ref{eq:nmr-ham-rf}.
% TODO change this reference to something in the rotating frame
% with two control amplitudes, makes much more sense
The spectrometer's electronics limit the strength and the granularity of the rf field,
but this still allows for a staggeringly large \emph{continuous} action space. Instead of allowing for continuous control of the rf field%
\footnote{
Which is precisely the action space used in gradient ascent pulse engineering (GRAPE) algorithms \cite{Khaneja-2005}, although it does not use RL.
}%
, we only consider a finite set of pulses, each with an interval of free evolution after the pulse. The set of pulses we use is the set of $\pi/2$ rotations about the x- or y-axis along with a ``null pulse'' or delay. The action set is represented as $\{ D, X, \overline{X}, Y, \overline{Y} \}$.



In the physical system, the actions correspond to unitary operators that determine the overall propagator. If the free evolution interval has length $\tau$, then the $D$ action corresponds to propagator
\[
D \longrightarrow U_{\tau} = \exp\{ -i H_{\text{int}} \tau \}
\]
And the other pulse actions have corresponding propagators
\begin{align*}
    X &\longrightarrow U_{\tau} U_X \\
    \overline{X} &\longrightarrow U_{\tau} U_{\overline{X}} \\
    Y & \longrightarrow U_{\tau} U_Y \\
    \overline{Y} &\longrightarrow U_{\tau} U_{\overline{Y}}
\end{align*}
Where $U_X$ is defined in equation~\ref{eq:X-pulse}. In the simulations, $\tau = 10^{-4}$ and the pulse width $t_p = 10^{-5}$ unless otherwise specified.
% TODO give any more justification to this?
% TODO mention Clifford gates/group?
See figure~\ref{fig:actions} for the relationship between rf field amplitudes, unitary operators, and the actions.

\begin{figure}[H]
    \centering
    % \scalebox{.8}{
    \input{tikz/Pulse-Action.tex}
    % }
    \caption{The rf field amplitudes, the unitary operators, and corresponding actions.}
    \label{fig:actions}
\end{figure}

In principle, the actions could also be defined as pulses without a $\tau$ delay of free evolution and a delay action. However, in practice this leads to worse performance: for episodes with a fixed number of timesteps, the actions taken influence how much time elapses in the pulse sequence. As a result, the ``optimal'' performance is found to minimize the elapsed time by applying alternating pulses (e.g. $X, \overline{X}, X, \overline{X}, \dots$).
The unitary operator fidelity generally improves as the time interval decreases, so this is an unhelpful solution when pursuing high fidelity at longer time intervals.

\subsection{State Space and Representation}

The ``state'' can refer to the physics notion of state--such as the the density operator $\rho$ of a quantum system--or the RL notion of state--a particular context in which the agent must choose an action. They are closely related but not the same.
The ``physical'' state space is comprised of every reachable state $\rho(t)$ of the spin system given an initial thermalized state, the free evolution dynamics from $H_{\text{int}}$, and the control Hamiltonian given by the rf field $H_{\text{rf}}$.
In an RL context, however, the agent does not observe the complete state of the system at each point in time. Instead, the agent only knows the sequence of actions that have been performed on the system and any measurements that have been made on the system.
Therefore, a more representative state space for the RL problem is all possible sequences of actions performed on the system.%
\footnote{
Making measurements on the system could also be considered an action (measurement affects the physical state), and the resulting measurement outcome could be included in the state \cite{porotti2019coherent}. This possibility, however, is not explored here.
}
This is more ``realistic'' in the sense that the agent only knows what the experimenter would know, without getting any additional information ``for free.''
%This also simplifies the state representation with ensembles of spin systems.
% TODO what did I mean by this^???

\subsection{Reward Function}

% The fidelity approaches $1$ as the unitaries get ``closer'' and the average Hamiltonian approaches the target Hamiltonian, and approaches $0$ as the unitaries get further apart.
As mentioned above, the fidelity function takes on values between $0$ and $1$, where values close to $1$ are desirable.
However, a tenfold improvement in performance doesn't correspond to a tenfold increase in fidelity (take for instance a fidelity of 0.9 compared to 0.99). A suitable reward function is then calculated by
\begin{equation}
    r = -\log(1-\text{fidelity}(U_\text{actual}, U_\text{target}) + \epsilon)
\end{equation}
where $\epsilon>0$ ensures the reward isn't infinite.

\subsection{AlphaZero Algorithm}

As mentioned in the introduction, the AlphaZero algorithm has two main components: self-play and neural network training. The original publication and its supplementary materials has a complete description of the algorithm \cite{Silver1140}, but pseudocode is presented below as well.

During self-play, the agent iteratively constructs a pulse sequence by performing Monte Carlo Tree Search (MCTS) from the current state. From state $s_i$ (which is represented by the sequence of actions performed so far, $s_i = (a_0, \dots, a_i)$), the agent stochastically samples actions based on the prior probability from the policy $\pi$, the number of times the agent has already ``visited'' that action, and the average estimated reward (or value) of that branch of the tree. The agent is more likely to select actions that have a higher prior probability, a lower visit count, and a higher average reward. The exact formula for selection is given in algorithm~\ref{al:mcts},
and is a variant of the PUCT algorithm from \cite{Rosin:2011uu}.

The agent performs many rollouts of selecting actions, estimating the rewards, and updating visit counts and reward estimates along the particular sequence of actions. After a certain number of rollouts, the agent finally picks a particular action to perform by sampling from the empirical distribution of visit counts.
See figure~\ref{fig:mcts} for a visual depiction of the MCTS process.


{\setstretch{1.0}
\begin{algorithm}[H]
\KwData{Current state $s = (a_0, \dots, a_{i-1})$, policy and value networks $(\pi, v)$, number of rollouts $n_\text{roll}$, Dirichlet noise parameter $\alpha$, noise weight $w \in [0,1]$, PUCT parameters $c_\text{base}, c_\text{init}$}
\KwResult{Probabilities $\{p_k\}_k$ of selecting action $a_k$ from state $s$, and randomly sampled action $a$}
\BlankLine
Initialize root node associated with current state $s$ \;
Create child nodes of the root node corresponding to each action in the action space \;
Initialize set of statistics $\{ N(s,a_k)=0, W(s,a_k)=0, Q(s,a_k)=0, P(s,a_k)= \pi(s, a_k) \}_k$ for each child node $a_k$, where $N$ is the visit count, $W$ is the total value, $Q$ is the mean value, and $P$ is the prior probability from the policy network $\pi$ \;

Sample multivariate Dirichlet noise from $\text{Dir}(\alpha)$ and add it to the priors $\{P(s, a_k)\}_k$ \;
\For{$n_\text{roll}$ tree rollouts}{
Set $s_0 \leftarrow s$, $i \leftarrow 0$ \;
\While{$s_i$ is not a leaf node}{
Select a child node of $s_i$ associated with action $a_k$ by maximizing $Q(s_i, a_k) + U(s_i, a_k)$ where
\[
U(s, a) = C(s) P(s,a) \sqrt{N(s)}/(1+N(s,a))
\]
And $C(s) = \log((1+N(s) + c_\text{base})/c_\text{base}) + c_\text{init}$ \;
Let $s_{i+1} \leftarrow s_i + (a_k)$ and $i \leftarrow i + 1$ \;
}

\If{
the leaf node $s_L$ represents a pulse sequence with the desired length
}{
Calculate the fidelity of the pulse sequence in spin system ensemble \;
Calculate the reward $r$ from fidelity \;
}
\Else{
Evaluate the leaf node $s_L$ using $(\pi, v)$ to create child nodes of $s_L$ with initialized statistics $\{ N(s_L,a_k)=0, W(s_L,a_k)=0, Q(s_L,a_k)=0, P(s_L,a_k)= \pi(s_L, a_k) \}_k$, and an estimated reward $r = v(s_L)$ \;
}
For each of the nodes $s_i$ visited from $s_0$ to $s_L$, update the visit counts $N(s_i, a_i) = N(s_i, a_i) + 1$ and the values $W(s_i, a_i) = W(s_i, a_i) + r, Q(s_i, a_i) = \frac{W(s_i, a_i)}{N(s_i, a_i)}$ \;
}

Calculate the estimated probability $p_k \leftarrow N(s, a_k) / N(s)$ for each action $a_k$ \;
Randomly sample an action $a_k$ from the distribution $\{ p_k \}$ \;

\caption{Monte Carlo Tree Search (MCTS) for selecting the next action. \label{al:mcts}}
\end{algorithm}
}

\begin{figure}[H]
    \centering
    % \includegraphics[width=.7\textwidth]{MCTS.jpeg}
    % \scalebox{.8}{
    \input{tikz/MCTS/MCTS-Visual.tex}
    % }
    \caption{An example Monte Carlo Tree Search (MCTS) starting with root state $s = (X)$. The first rollout explores the action branch $D$, creating each of the child nodes for $D$. The next rollout instead explores $\overline{Y}$ starting from $(X)$. The final two rollouts depicted choose $(\overline{Y}, D)$ and $(\overline{Y}, D, Y)$ from $(X)$. The rollouts continue for a set number of rollouts, and the next action selected from $(X)$ is sampled proportionally according to visit counts of each of the children nodes.}
    \label{fig:mcts}
\end{figure}

The agent will repeat the process of MCTS and action sampling until it has constructed a pulse sequence with the desired length. The agent records search statistics from each step of the pulse sequence construction: the state $s_i$, the empirical probability distribution of actions explored from MCTS, and the final reward $r$ of the pulse sequence from simulation. These statistics are then used to train the policy and value neural networks $(\pi, v)$.
% TODO make sure this doesn't drop off on new page...


\begin{algorithm}[H]
\KwData{Pulse sequence length $T$}
\KwResult{Search statistics $S$}
\BlankLine
Initialize empty search statistics $S \leftarrow \{\}$
and initial state $s_0 \leftarrow ()$
\;
\While{Pulse sequence length is less than $T$}{
Perform MCTS from state $s_i$ to get probabilities of selecting action $k$ ($\{p_k\}^{(i)}$) and randomly sample the next action $a_i$ \;

Append $(s_i, \{p_k\}^{(i)})$ to the search statistics $S$ \;
Apply action $a_i$, get updated state $s_{i+1} \leftarrow (a_0, \dots, a_i)$ \;
}
Calculate the reward $r$ for the final pulse sequence \;
Include the reward $r$ in each element in the search statistics $S$, so $S = \{ (s_i, \{p_k\}^{(i)}, r) \}$ \;
\caption{Pulse sequence construction via self-play. \label{al:make_sequence}}
\end{algorithm}

As the agent continually generates search statistics from self-play, the policy and value neural networks $(\pi, v)$ in turn are trained on the collected data. The loss function to train the neural networks is
\begin{equation}\label{eq:az_loss}
    L(\theta) = (r_i - v_\theta(s_i))^2 - \pi_\theta(s_i) \cdot \log \mathbf{p_i} + c ||\theta||^2
\end{equation}
The first term is the squared error of the reward estimates, the second is the cross-entropy loss of the policy estimates, and the third is $L_2$ regularization of the neural network parameters. The constant $c$ controls the weight of $L_2$ regularization (in this work $c = 10^{-6}$).

The two processes of self-play and training are done simultaneously, with multiple agent processes constructing pulse sequences and generating data that is then used to train the policy and value networks. In turn, the updated network parameters $\theta$ are shared with the agents.
% The flow of information is shown in figure~\ref{fig:az-parallel}.
% TODO make az-parallel figure if I have time!!!


% \begin{figure}[H]
%     \centering
%     \includegraphics[width=.8\textwidth]{az-parallel.jpeg}
%     \caption{The parallelized execution of the AlphaZero algorithm. Many agent processes construct pulse sequences and feed the generated data to a training process. The updated network parameters $\theta$ are then shared with the agents.}
%     \label{fig:az-parallel}
% \end{figure}
% TODO do this if I have time!!!

\subsection{Neural Network Architecture}

As in the original AlphaZero implementation, the policy and the value networks were combined into a single neural network that maps a state $s$ to \emph{both} a probability distribution over actions $\pi(s)$ and a value estimate $v(s)$. This is written as $f_\theta(s) = (\pi(s), v(s))$, where $\theta$ are the neural network parameters. This adds an additional degree of regularization, because any parameters that are used for both the policy and value estimates must minimize the loss for both the policy and the value estimates, preventing overfitting. The combined network includes a shared set of layers that transform the state to an intermediate representation, and two separate sets of layers (called ``heads'') that return the policy and value estimates. See figure~\ref{fig:nn} for the complete neural network architecture.

The state is represented by the sequence of actions performed on the system and an additional \texttt{START} ``action.''
Because the action space is discrete ($\{ D, X, \overline{X}, Y, \overline{Y} \}$), each action is represented by a one-hot vector%
\footnote{
A one-hot vector has a single entry equal to one, and all other entries are zero.
}%
. For example, the \texttt{START} action corresponds to the vector $[1,0,0,0,0]$, a delay $D$ corresponds to the vector $[0,1,0,0,0,0]$, and $\overline{Y}$ corresponds to $[0,0,0,0,0,1]$. The sequence of pulses $[\overline{X}, Y, D]$ is then represented by the 2D-array
\begin{align*}
    [&[1,0,0,0,0,0], \\
    &[0,0,0,1,0,0], \\
    &[0,0,0,0,1,0], \\
    &[0,1,0,0,0,0]]
\end{align*}
Because the state is represented by a sequence of actions, the size of the state depends on the length of the pulse sequence.
The policy head outputs a $1 \times 5$ vector of probabilities associated with each of the possible actions. The value head outputs a single scalar for the estimated reward of the pulse sequence so far.

The state representation--as a sequence of actions--lends itself to using a recurrent neural network architecture. A gated recurrent unit (GRU) was used, though a long-short term memory (LSTM) layer performed similarly \cite{cho2014learning, lstm}.
Both GRUs and LSTMs are capable of mapping variable-length input to a fixed-length output space, which is precisely what is needed when mapping the actions we have performed so far to the estimated policy and value of that state.


\section{AHT-Constrained Search}\label{sec:AHT-constraints}

% \smash{
Because actions are explored and selected via tree search, additional constraints can be added to prune branches of the tree that are unlikely to yield high rewards.
This is akin to adding ``rules of the game'' that determine which actions can or cannot be applied.
Thankfully, AHT gives us a straightforward constraint so that pulse sequences have the desired lowest-order average Hamiltonian $\overline{H}^{(0)}$.
% }

When the target effective Hamiltonian $H_\text{eff} = 0$, all interactions must be decoupled. If the toggling frame is defined so that $\widetilde{I}_z(t) = I_z$ and $\widetilde{I}_z(t) = -I_z$ for equal durations, then the lowest-order average $\overline{I}_z^{(0)} = 1/2(I_z - I_z) = 0$. In general, any term containing $I_z$ will average to zero to lowest order when $\widetilde{I}_z$ is toggled for equal time along opposite axes (i.e. equal time on $+x$ and $-x$, $+y$ and $-y$, and $+z$ and $-z$-axes).
Doing so would decouple the chemical shift term $H_\text{CS}$ to lowest order.

We saw in section~\ref{subsec:WHH-4} that the dipolar interaction term is decoupled when $I_z^{(j)}I_z^{(k)}$ is toggled to $I_z^{(j)}I_z^{(k)}, I_y^{(j)}I_y^{(k)}$, and $I_x^{(j)}I_x^{(k)}$ for equal times.
Note that if $\widetilde{I}_z = -I_z$, then $\widetilde{I}_z^{(j)}\widetilde{I}_z^{(k)} = (-I_z^{(j)})(-I_z^{(k)}) = I_z^{(j)}I_z^{(k)}$, so the overall term remains unchanged.
Therefore the dipolar interaction is decoupled to lowest order when $I_z^{(j)}I_z^{(k)}$ is toggled for equal time along the $x, y$, and $z$-axes.

To decouple all interactions, both constraints above must be satisfied, which is achieved when $I_z$ is toggled along each of the six axes for equal time ($\pm x, \pm y, \pm z$). By keeping track of the toggled operator $\widetilde{I}_z$ while constructing the pulse sequence, it is possible to prune branches of the tree that would violate this constraint%
% (see figure~\ref{fig:tree-prune})%
.

% TODO include if I have time...
% \begin{figure}[H]
%     \centering
%     \includegraphics[width=.5\textwidth]{example-image}
%     \caption{Branches of the pulse sequence tree are pruned using the lowest-order average Hamiltonian constraints.}
%     \label{fig:tree-prune}
% \end{figure}


% TODO include helpful table with all the parameters?
% physical system parameters
% - number of spins
% - chemical shift
% - dipolar coupling strength
% - pulse width
% - delay between pulses
% - rotation errors
% - offset errors
% - phase transient errors
% RL algorithm parameters
% - pulse sequence length/episode length
% - num_simulations
% - Dirichlet noise parameter
% - exploration fraction
% - PUCB base, init
% - batch size
% - number of agents
% - learning rate
% - L2 weight
% - value weight
% - tree constraints
% - neural network architecture
